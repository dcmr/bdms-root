package com.bdms.spark

import org.apache.spark.streaming.StreamingContext
import org.apache.spark.SparkConf
import org.apache.spark.streaming.Seconds
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming._
import org.apache.spark.SparkContext._
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming.dstream._
import org.apache.spark.SparkContext
import org.apache.spark.storage.StorageLevel
import org.apache.spark.HashPartitioner
import com.bdms.spark.util.SparkUtil
import com.bdms.spark.customreceiver.CustomSocketReceiver

/**
 * Counts words cumulatively in UTF8 encoded, '\n' delimited text received from the network every
 * second starting with initial value of word count.
 * Usage: StatefulNetworkWordCount <hostname> <port>
 *   <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive
 *   data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example
 *      org.apache.spark.examples.streaming.StatefulNetworkWordCount localhost 9999`
 */
object SocketTest {
  
  def main(args: Array[String]) {
   
    val sparkConf = SparkUtil.getSparkConf("socketTest", "")
    // Create the context with a 1 second batch size
    val ssc = new StreamingContext(sparkConf, Seconds(30))

    // Create a ReceiverInputDStream on target ip:port and count the
    // words in input stream of \n delimited test (eg. generated by 'nc')
    
    val lines =  ssc.receiverStream(new CustomSocketReceiver("192.168.66.207", 12345))
    lines.print()
    
    
    ssc.start()
    ssc.awaitTermination()
  }
}